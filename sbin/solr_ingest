#!/usr/bin/env python

"""
solr_admin -- manage Solr cores

@copyright:  2015 FU Berlin. All rights reserved.
        
@contact:    sebastian.illing@met.fu-berlin.de

@license:    BSD

Copyright (c) 2015, FU Berlin
All rights reserved.

Redistribution and use in source and binary forms, with or without modification,
 are permitted provided that the following conditions are met:

    Redistributions of source code must retain the above copyright notice, this 
    list of conditions and the following disclaimer.
    Redistributions in binary form must reproduce the above copyright notice, 
    this list of conditions and the following disclaimer in the documentation 
    and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND 
ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED 
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. 
IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, 
INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, 
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, 
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY 
OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE 
OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED 
OF THE POSSIBILITY OF SUCH DAMAGE.
"""

from evaluation_system.commands import FrevaBaseCommand, CommandError
import evaluation_system.model.file as mf
from evaluation_system.model.solr_core import SolrCore
import logging, sys
from evaluation_system.model.solr_models.models import UserCrawl
from evaluation_system.model.user import User


class SolrIngest(FrevaBaseCommand):
    _short_args = "hd"
    _args = [
        {
            "name": "--debug",
            "help": "turn on debugging info and show stack trace on exceptions.",
            "action": "store_true",
        },
        {"name": "--help", "help": "shows help info", "action": "store_true",},
        {
            "name": "--crawl",
            "help": "which directory to crawl to generate a digest",
            "metavar": "DIRECTORY",
        },
        {
            "name": "--ingest",
            "help": "digest file to submit to solr",
            "metavar": "FILE",
        },
        {
            "name": "--output",
            "help": "output location of solr crawl digest",
            "metavar": "FILE",
        },
        {
            "name": "--batch-size",
            "help": "batch size to submit to solr",
            "metavar": "NUMBER",
            "default": 200,
        },
        {"name": "--solr-url", "help": "url of solr server", "metavar": "URL",},
    ]
    __short_description__ = (
        """\nCommand to ingest files to solr or dump crawl to file"""
    )

    def handle_exceptions(self, e):
        if hasattr(self, "ingest_file"):
            try:
                crawl = UserCrawl.objects.get(tar_file=self.ingest_file.split("/")[-1])
                crawl.status = "failed"
                crawl.ingest_msg = (
                    crawl.ingest_msg
                    + "\n \n"
                    + str(e)
                    + "\nIngesting failed. Please check your directory structure."
                )
                crawl.save()
            except:
                pass

    def _run(self):
        # defaults
        batch_size = 10000
        crawl_dir = None
        abort_on_errors = False
        output = None
        host = None
        port = None

        crawl_dir = self.args.crawl
        ingest_file = self.args.ingest
        batch_size = self.args.batch_size

        solr_url = self.args.solr_url
        import re

        mo = re.match(r"(?:https?://)?([^:/]{1,})(?::([0-9]{1,}))?(?:/.*|$)", solr_url)
        if not mo:
            raise Exception("Cannot understand the solr-url %s" % solr_url)
        host = mo.group(1)
        port = int(mo.group(2))

        output = self.args.output

        if self.args.debug:
            abort_on_errors = True
            DEBUG = True
            logging.getLogger().setLevel(logging.DEBUG)

        if crawl_dir is None and ingest_file is None:
            raise CommandError(
                "You must either crawl to generate a dump file or ingest it"
            )

        # flush stderr in case we have something pending
        sys.stderr.flush()
        if host:
            core_files = SolrCore(core="files", host=host, port=port)
            core_latest = SolrCore(core="latest", host=host, port=port)

        if crawl_dir:
            if not output:
                raise Exception("`output` file argument required when creating digest")
            SolrCore.dump_fs_to_file(
                crawl_dir,
                output,
                batch_size=batch_size,
                abort_on_errors=abort_on_errors,
            )
            # creata database entry
            user = User()
            db = user.getUserDB()
            UserCrawl.objects.create(
                status="crawling",
                path_to_crawl=crawl_dir,
                user_id=db.getUserId(user.getName()),
                tar_file=output.split("/")[-1],
            )
        elif ingest_file:
            self.ingest_file = ingest_file
            from evaluation_system.misc.utils import capture_stdout

            # from evaluation_system.model.solr_models.models import UserCrawl
            fn = ingest_file.split("/")[-1]
            UserCrawl.objects.filter(tar_file=fn).update(status="ingesting")
            with capture_stdout() as capture:
                # Ingest the files!
                if host:
                    SolrCore.load_fs_from_file(
                        dump_file=ingest_file,
                        batch_size=batch_size,
                        abort_on_errors=abort_on_errors,
                        core_all_files=core_files,
                        core_latest=core_latest,
                    )
                else:
                    SolrCore.load_fs_from_file(
                        dump_file=ingest_file,
                        batch_size=batch_size,
                        abort_on_errors=abort_on_errors,
                    )
            print(capture.result)
        try:
            crawl = UserCrawl.objects.get(tar_file=fn)
            crawl.ingest_msg = (
                crawl.ingest_msg
                + "\n"
                + capture.result
                + '\n\nNow you can find your data using "solr_search"'
            )
            crawl.status = "success"
            crawl.save()
        except:
            pass


if __name__ == "__main__":
    SolrIngest().run()
