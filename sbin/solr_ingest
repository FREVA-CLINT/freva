#!/usr/bin/env python

'''
solr_admin -- manage Solr cores

@copyright:  2015 FU Berlin. All rights reserved.
        
@contact:    sebastian.illing@met.fu-berlin.de

@license:    BSD

Copyright (c) 2015, FU Berlin
All rights reserved.

Redistribution and use in source and binary forms, with or without modification,
 are permitted provided that the following conditions are met:

    Redistributions of source code must retain the above copyright notice, this 
    list of conditions and the following disclaimer.
    Redistributions in binary form must reproduce the above copyright notice, 
    this list of conditions and the following disclaimer in the documentation 
    and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND 
ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED 
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. 
IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, 
INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, 
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, 
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY 
OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE 
OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED 
OF THE POSSIBILITY OF SUCH DAMAGE.
'''

from evaluation_system.commands import BaseCommand, CommandError
import evaluation_system.model.file as mf
from evaluation_system.model.solr_core import SolrCore
import logging,sys
from evaluation_system.model.solr_models.models import UserCrawl
from evaluation_system.model.user import User

class SolrIngest(BaseCommand):
    _short_args = 'hd'
    _args = ['debug', 'help', 'crawl=', 'ingest=', 'output=', 'batch-size=', 'solr-url=']
    __short_description__ = '''\nCommand to ingest files to solr or dump crawl to file'''    

    def handle_exceptions(self,e):
        if hasattr(self,'ingest_file'):
            try:
                crawl = UserCrawl.objects.get(tar_file=self.ingest_file.split('/')[-1])
                crawl.status = 'failed'
                crawl.ingest_msg = crawl.ingest_msg + '\n \n' + str(e) + '\nIngesting failed. Please check your directory structure.'
                crawl.save()
            except:
                pass

    def _run(self):
        #defaults
        batch_size=10000
        crawl_dir=None
        abort_on_errors=False
        output = None
        host = None
        port = None
        #parse arguments *!!!*
        for flag, arg in self.args:
            if flag == '--crawl':                 #crawl the given directory
                crawl_dir=arg
            elif flag == '--ingest':                #ingest the given file (as created by crawl)
                ingest_file=arg
            elif flag == '--batch-size':            #Number of entries to send at the same time to Solr.
                batch_size=int(arg)
            elif flag == '--solr-url':              #url to solr instance
                import re
                mo = re.match('(?:https?://)?([^:/]{1,})(?::([0-9]{1,}))?(?:/.*|$)', arg)
                if not mo:
                    raise Exception("Cannot understand the solr-url %s" % arg)
                host= mo.group(1)
                port = int(mo.group(2))
            elif flag == '--output':                #Instead of ingesting into Solr write to this file
                output = arg
            elif flag == '-d' or flag == '--debug': #turn on debugging info
                abort_on_errors=True
                DEBUG = True
                logging.getLogger().setLevel(logging.DEBUG)
        if crawl_dir is None and ingest_file is None:
            raise CommandError('You must either crawl to generate a dump file or ingest it')

        #flush stderr in case we have something pending
        sys.stderr.flush()
        if host:
            core_files = SolrCore(core='files', host=host, port=port)
            core_latest = SolrCore(core='latest', host=host, port=port)

        if crawl_dir:
            if not output:
                raise Exception("You need to dump a file")
            SolrCore.dump_fs_to_file(crawl_dir, output, batch_size=batch_size, abort_on_errors=abort_on_errors)
            #creata database entry
            user=User()
            db = user.getUserDB()
            UserCrawl.objects.create(status='crawling',path_to_crawl=crawl_dir, user_id=db.getUserId(user.getName()), tar_file=output.split('/')[-1])
        elif ingest_file:
            self.ingest_file = ingest_file
            from evaluation_system.misc.utils import capture_stdout
            #from evaluation_system.model.solr_models.models import UserCrawl
            fn = ingest_file.split('/')[-1]
            UserCrawl.objects.filter(tar_file=fn).update(status='ingesting')
            with capture_stdout() as capture:
                #Ingest the files!
                if host:
                    SolrCore.load_fs_from_file(dump_file=ingest_file, batch_size=batch_size, abort_on_errors=abort_on_errors,
                                           core_all_files=core_files, core_latest=core_latest)
                else:
                    SolrCore.load_fs_from_file(dump_file=ingest_file, batch_size=batch_size, abort_on_errors=abort_on_errors)
            print(capture.result)
        try:
                crawl = UserCrawl.objects.get(tar_file=fn)
                crawl.ingest_msg = crawl.ingest_msg + '\n' + capture.result + '\n\nNow you can find your data using "solr_search"'
                crawl.status = 'success'
                crawl.save()
        except:
                pass

if __name__ == "__main__":
    SolrIngest().run()
